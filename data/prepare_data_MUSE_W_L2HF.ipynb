{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "x_l2 = np.loadtxt('./wide/matter_power_564_Box250_Part750_21_Box1000_Part3000_z0/train_input_fidelity_0.txt')\n",
    "\n",
    "\n",
    "# HF input and output\n",
    "x_h = np.loadtxt('./wide/matter_power_564_Box250_Part750_21_Box1000_Part3000_z0/train_input_fidelity_1.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all z \n",
    "z_str = ['0', '0.2', '0.5', '1', '2', '3']\n",
    "\n",
    "y_l2 = []\n",
    "y_h = []\n",
    "for i in range(len(z_str)):\n",
    "    y_l2.append(np.loadtxt('./wide/matter_power_564_Box250_Part750_21_Box1000_Part3000_z' + z_str[i] + '/train_output_fidelity_0.txt'))\n",
    "    y_h.append(np.loadtxt('./wide/matter_power_564_Box250_Part750_21_Box1000_Part3000_z' + z_str[i] + '/train_output_fidelity_1.txt'))\n",
    "y_h = np.concatenate(y_h, axis=1)\n",
    "y_l2 = np.concatenate(y_l2, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 384)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(564, 384)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_l2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 384)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the L1 input and find the ones (indices) that are in the HF input\n",
    "indices = []\n",
    "for i in range(len(x_l2)):\n",
    "    if x_l2[i] in x_h:\n",
    "        indices.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24,\n",
       " 25,\n",
       " 26,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 522,\n",
       " 523,\n",
       " 524]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the ratio\n",
    "r_h = 10**y_h / 10**y_l2[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_l2 = np.concatenate((x_l2[indices], y_l2[indices]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 394)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_l2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'muse_L2H/kf.txt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the dataset\n",
    "save_dir_L = 'muse_L2'\n",
    "save_dir_LHr = 'muse_L2Hr'\n",
    "save_dir_LH = 'muse_L2H'\n",
    "\n",
    "# mkdir -p the dirs\n",
    "import os\n",
    "os.makedirs(save_dir_L, exist_ok=True)\n",
    "os.makedirs(save_dir_LHr, exist_ok=True)\n",
    "os.makedirs(save_dir_LH, exist_ok=True)\n",
    "\n",
    "np.savetxt(save_dir_L + '/train_input.txt', x_l2)\n",
    "np.savetxt(save_dir_L + '/train_output.txt', y_l2)\n",
    "\n",
    "np.savetxt(save_dir_LHr + '/train_input.txt', x_h)\n",
    "np.savetxt(save_dir_LHr + '/train_output.txt', r_h)\n",
    "\n",
    "np.savetxt(save_dir_LH + '/train_input.txt', xy_l2)\n",
    "np.savetxt(save_dir_LH + '/train_output.txt', y_h)\n",
    "\n",
    "# copy kf.txt\n",
    "import shutil\n",
    "shutil.copyfile('./wide/matter_power_564_Box250_Part750_21_Box1000_Part3000_z0/kf.txt', save_dir_L + '/kf.txt')\n",
    "shutil.copyfile('./wide/matter_power_564_Box250_Part750_21_Box1000_Part3000_z0/kf.txt', save_dir_LHr + '/kf.txt')\n",
    "shutil.copyfile('./wide/matter_power_564_Box250_Part750_21_Box1000_Part3000_z0/kf.txt', save_dir_LH + '/kf.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PCs: 21\n",
      "Explained variance: 0.999991656153113\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_decomp(y, explained_min=0.999, standardize=False):\n",
    "        pca = PCA()\n",
    "        pca.fit(y)\n",
    "        # Explained variance ratio for each PC\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "        # Cumulative explained variance\n",
    "        cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "        # Find the number of PCs that explain args.min_pca of the variance\n",
    "        n_components = np.argmax(cumulative_variance > explained_min) + 1\n",
    "\n",
    "        # Transform the data, only keep the first n_components\n",
    "        y = pca.transform(y)[:, :n_components]\n",
    "\n",
    "        # standardize the output data (PCA coefficients) (not recommended)\n",
    "        if standardize:\n",
    "            y_mean = np.mean(y, axis=0)\n",
    "            y_std = np.std(y, axis=0)\n",
    "            y = (y - y_mean) / y_std\n",
    "        else:\n",
    "            y_mean = None\n",
    "            y_std = None\n",
    "\n",
    "        # save the PCA components\n",
    "        mean_std = {'pca_components': pca.components_[:n_components], 'pca_mean': pca.mean_, 'mean': y_mean, 'std': y_std}\n",
    "\n",
    "        # print the number of PCs and the explained variance\n",
    "        print(f\"Number of PCs: {n_components}\")\n",
    "        print(f\"Explained variance: {cumulative_variance[n_components - 1]}\")\n",
    "\n",
    "        return y, mean_std\n",
    "\n",
    "# PCA global\n",
    "y_l2_pca_g, mean_std_g = pca_decomp(y_l2, explained_min=.99999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_l2_pca_g = np.concatenate((x_l2[indices], y_l2_pca_g[indices]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 31)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_l2_pca_g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'muse_L2H_pca_g/kf.txt'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save L1H_pca_g\n",
    "save_dir_LH_pca_g = 'muse_L2H_pca_g'\n",
    "os.makedirs(save_dir_LH_pca_g, exist_ok=True)\n",
    "np.savetxt(save_dir_LH_pca_g + '/train_input.txt', xy_l2_pca_g)\n",
    "np.savetxt(save_dir_LH_pca_g + '/train_output.txt', y_h)\n",
    "\n",
    "shutil.copyfile('./wide/matter_power_564_Box250_Part750_21_Box1000_Part3000_z0/kf.txt', save_dir_LH_pca_g + '/kf.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PCs: 10\n",
      "Explained variance: 0.9999923957378889\n",
      "Number of PCs: 9\n",
      "Explained variance: 0.9999909390884748\n",
      "Number of PCs: 9\n",
      "Explained variance: 0.99999263128643\n",
      "Number of PCs: 8\n",
      "Explained variance: 0.9999919136131744\n",
      "Number of PCs: 7\n",
      "Explained variance: 0.9999903121700564\n",
      "Number of PCs: 7\n",
      "Explained variance: 0.9999919788750318\n"
     ]
    }
   ],
   "source": [
    "# pca local\n",
    "n_pc_zs = []\n",
    "y_pca = []\n",
    "n_z = len(z_str)\n",
    "n_k = len(np.loadtxt('./wide/matter_power_564_Box250_Part750_21_Box1000_Part3000_z0/kf.txt'))\n",
    "# y = y.reshape(-1, n_z, n_k)\n",
    "for i in range(n_z):\n",
    "    ik_start = i * n_k\n",
    "    ik_end = (i + 1) * n_k\n",
    "    y_pca_z, mean_std_z = pca_decomp(y_l2[:, ik_start:ik_end], explained_min=.99999)\n",
    "    # n_pc_zs.append(y_pca_z.shape[1])\n",
    "    y_pca.append(y_pca_z)\n",
    "\n",
    "# concatenate the PCA coefficients\n",
    "y_l2_pca_l = np.concatenate(y_pca, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(564, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_l2_pca_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_l2_pca_l = np.concatenate((x_l2[indices], y_l2_pca_l[indices]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'muse_L2H_pca_l/kf.txt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save L1H_pca_l\n",
    "save_dir_LH_pca_l = 'muse_L2H_pca_l'\n",
    "os.makedirs(save_dir_LH_pca_l, exist_ok=True)\n",
    "np.savetxt(save_dir_LH_pca_l + '/train_input.txt', xy_l2_pca_l)\n",
    "np.savetxt(save_dir_LH_pca_l + '/train_output.txt', y_h)\n",
    "\n",
    "shutil.copyfile('./wide/matter_power_564_Box250_Part750_21_Box1000_Part3000_z0/kf.txt', save_dir_LH_pca_l + '/kf.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
